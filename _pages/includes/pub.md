# üìù Representative Publications 
## Mutli-modal Representation Learning
- Unified Representations: **C-MCR (NeurIPS 2023)**, **Ex-MCR**
- Audio-Video Representations: **LiMo**
- Visual-Language Representations: **DG-NLVL (ACL 2023)**

## 3D-Language Understanding
- Large Language Model for 3D: **Chat-3D**
- 3D Visual Grounding: **3DRP-Net (EMNLP 2023)**, **Weakly supervised 3DVG (ICCV 2023)**

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2023</div><img src='images/Ex-MCR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [Extending Multi-modal Contrastive Representations.](https://arxiv.org/abs/2310.08884) **Zehan Wang**, Ziang Zhang, Luping Liu, Yang Zhao, Haifeng Huang, Tao Jin, Zhou Zhao **Arxiv, 2023**
- Learn unified multimodal contrastive representations for more than three modalities in a paired-data free and training-efficient way.
- **Academic / Industry Impact**: Our code and pre-trained models are released at [![](https://img.shields.io/github/stars/MCR-PEFT/Ex-MCR?style=social&label=Code+Stars)](https://github.com/MCR-PEFT/Ex-MCR), which provides state-of-the-art unified 3D-image-text-audio representations.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2023</div><img src='images/chat3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">![img.png](img.png)

- [Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes.](https://arxiv.org/abs/2308.08769) **Zehan Wang**, Haifeng Huang, Yang Zhao, Ziang Zhang, Zhou Zhao **Arxiv, 2023**
- LLM for 3D, the frist universal dialogue system for 3D world.
- **Academic / Industry Impact**: Our code is released at [![](https://img.shields.io/github/stars/Chat-3D/Chat-3D?style=social&label=Code+Stars)](https://github.com/Chat-3D/Chat-3D).
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='images/C-MCR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [Connecting Multi-modal Contrastive Representations.](https://arxiv.org/abs/2305.14381) **Zehan Wang**, Yang Zhao, Xize Cheng, Haifeng Huang, Jiageng Liu, Li Tang, Linjun Li, Yongqi Wang, Aoxiong Yin, Ziang Zhang, Zhou Zhao **NeurIPS 2023**
- Paired-data free and training-efficient multi-modal constrastive representations learning method. 
- **Academic / Industry Impact**: Our work is reported by [PaperWeekly](https://mp.weixin.qq.com/s/n4RUkRTQsCHg0O-qWDzD9g). Our code and pre-trained models are released at [![](https://img.shields.io/github/stars/MCR-PEFT/C-MCR?style=social&label=Code+Stars)](https://github.com/MCR-PEFT/C-MCR), which provides state-of-the-art audio-visual and 3D-language representations.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/ICCV23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding.](https://arxiv.org/abs/2307.09267) **Zehan Wang**, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao **ICCV 2023**
- The first weakly-supervised 3D visual grounding method. 
</div>
</div>


# Full Publication List

## Multi-modal Representation Learning

## 3D-language Understanding
