# üìù Representative Publications 
## Mutli-modal Representation Learning
- Unified Representations: **[C-MCR](https://arxiv.org/abs/2305.14381) (NeurIPS 2023)**, **[Ex-MCR](https://arxiv.org/abs/2310.08884)**
- Audio-Video Representations: **LiMo**
- Visual-Language Representations: **[DG-NLVL](https://aclanthology.org/2023.findings-acl.11/) (ACL 2023)**

## 3D-Language Understanding
- Large Language Model for 3D: **[Chat-3D](https://arxiv.org/abs/2308.08769)**
- 3D Visual Grounding: **[3DRP-Net](https://arxiv.org/abs/2307.13363) (EMNLP 2023)**, **[Weakly supervised 3DVG](https://arxiv.org/abs/2307.09267) (ICCV 2023)**

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2023</div><img src='images/Ex-MCR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [Extending Multi-modal Contrastive Representations.](https://arxiv.org/abs/2310.08884) **Zehan Wang**, Ziang Zhang, Luping Liu, Yang Zhao, Haifeng Huang, Tao Jin, Zhou Zhao **Arxiv, 2023**
- Learn unified multimodal contrastive representations for more than three modalities in a paired-data free and training-efficient way.
- **Academic / Industry Impact**: Our code and pre-trained models are released at [![](https://img.shields.io/github/stars/MCR-PEFT/Ex-MCR?style=social&label=Code+Stars)](https://github.com/MCR-PEFT/Ex-MCR), which provides state-of-the-art unified 3D-image-text-audio representations.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2023</div><img src='images/chat3d.png' alt="sym" width="50%"></div></div>
<div class='paper-box-text' markdown="1">

- [Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes.](https://arxiv.org/abs/2308.08769) **Zehan Wang**, Haifeng Huang, Yang Zhao, Ziang Zhang, Zhou Zhao **Arxiv, 2023**
- LLM for 3D, the frist universal dialogue system for 3D world.
- **Academic / Industry Impact**: Our code is released at [![](https://img.shields.io/github/stars/Chat-3D/Chat-3D?style=social&label=Code+Stars)](https://github.com/Chat-3D/Chat-3D).
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='images/C-MCR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [Connecting Multi-modal Contrastive Representations.](https://arxiv.org/abs/2305.14381) **Zehan Wang**, Yang Zhao, Xize Cheng, Haifeng Huang, Jiageng Liu, Li Tang, Linjun Li, Yongqi Wang, Aoxiong Yin, Ziang Zhang, Zhou Zhao **NeurIPS 2023**
- Paired-data free and training-efficient multi-modal constrastive representations learning method. 
- **Academic / Industry Impact**: Our work is reported by [PaperWeekly](https://mp.weixin.qq.com/s/n4RUkRTQsCHg0O-qWDzD9g). Our code and pre-trained models are released at [![](https://img.shields.io/github/stars/MCR-PEFT/C-MCR?style=social&label=Code+Stars)](https://github.com/MCR-PEFT/C-MCR), which provides state-of-the-art audio-visual and 3D-language representations.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/ICCV23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- [Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding.](https://arxiv.org/abs/2307.09267) **Zehan Wang**, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao **ICCV 2023**
- The first weakly-supervised 3D visual grounding method. 
</div>
</div>


# Full Publication List

## 2023
- [Extending Multi-modal Contrastive Representations.](https://arxiv.org/abs/2310.08884) **Zehan Wang**, Ziang Zhang, Luping Liu, Yang Zhao, Haifeng Huang, Tao Jin, Zhou Zhao. Arxiv, 2023
- [Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes.](https://arxiv.org/abs/2308.08769) **Zehan Wang**, Haifeng Huang, Yang Zhao, Ziang Zhang, Zhou Zhao. Arxiv, 2023
- [Connecting Multi-modal Contrastive Representations.](https://arxiv.org/abs/2305.14381) **Zehan Wang**, Yang Zhao, Xize Cheng, Haifeng Huang, Jiageng Liu, Li Tang, Linjun Li, Yongqi Wang, Aoxiong Yin, Ziang Zhang, Zhou Zhao. NeurIPS 2023
- [Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding.](https://arxiv.org/abs/2307.09267) **Zehan Wang**, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao. ICCV 2023
- [MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition]() Xize Cheng, Tao Jin, Rongjie Huang, Linjun Li, Wang Lin, **Zehan Wang**, Ye Wang, Huadai Liu, Aoxiong Yin, Zhou Zhao. ICCV 2023
- [3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding]() Zehan Wang, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao. EMNLP 2023
- [Scene-robust natural language video localization via learning domain-invariant representations]() Zehan Wang, Yang Zhao, Haifeng Huang, Yan Xia, Zhou Zhao. ACL 2023